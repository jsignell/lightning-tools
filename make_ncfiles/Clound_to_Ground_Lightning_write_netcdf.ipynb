{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create daily netcdf files for the period 1991 - 2015\n",
    "There are two types of files. The new ones have a bunch of data variables and the old ones just have time, lat, lon, amplitude, and strokes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_path = '/run/media/jsignell/WRF/Data/LIGHT/Data_1991-2009/'\n",
    "new_path = '/run/media/jsignell/WRF/Data/LIGHT/raw/'\n",
    "out_path = '/home/jsignell/erddapData/Cloud_to_Ground_Lightning/US/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('messed_up_old_files.txt')\n",
    "l = f.readlines()\n",
    "l=[fname.strip() for fname in l]\n",
    "f.close()\n",
    "l.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fname_to_ncfile(fname, old=False, new=True):\n",
    "    if new:\n",
    "        tstr = '{y}-{doy}'.format(y=fname[6:10], doy=fname[11:14])\n",
    "        ncfile = str(pd.datetime.strptime(tstr, '%Y-%j').date()).replace('-','_')+'.nc'\n",
    "        return(ncfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see how we are doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1991-02-01',\n",
       " '1991-12-05',\n",
       " '1992-01-17',\n",
       " '1992-12-18',\n",
       " '1992-12-21',\n",
       " '1993-10-31',\n",
       " '1994-07-31',\n",
       " '1997-10-12',\n",
       " '1997-10-13',\n",
       " '1998-11-23',\n",
       " '2000-01-06',\n",
       " '2002-02-25',\n",
       " '2003-12-31',\n",
       " '2004-02-08',\n",
       " '2006-02-08',\n",
       " '2007-01-16',\n",
       " '2007-02-06',\n",
       " '2007-12-18',\n",
       " '2008-12-05',\n",
       " '2009-01-17',\n",
       " '2013-01-16']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d for d in pd.date_range('1991-01-01','2015-09-30').astype(str) if (d+'.nc').replace('-','_') not in os.listdir(out_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1991: 363,\n",
       " 1992: 363,\n",
       " 1993: 364,\n",
       " 1994: 364,\n",
       " 1995: 365,\n",
       " 1996: 366,\n",
       " 1997: 363,\n",
       " 1998: 364,\n",
       " 1999: 365,\n",
       " 2000: 365,\n",
       " 2001: 365,\n",
       " 2002: 364,\n",
       " 2003: 364,\n",
       " 2004: 365,\n",
       " 2005: 365,\n",
       " 2006: 364,\n",
       " 2007: 362,\n",
       " 2008: 365,\n",
       " 2009: 364,\n",
       " 2010: 365,\n",
       " 2011: 365,\n",
       " 2012: 366,\n",
       " 2013: 364,\n",
       " 2014: 365,\n",
       " 2015: 273}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {}\n",
    "for y in range(1991, 2016):\n",
    "    d.update({y: len([f for f in os.listdir(out_path) if str(y) in f])})\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking doesn't work well if there are too few records. For some reason only the time values were written to the files in these cases. In order to make up for this, we will find all the really small files and then get the year and day of year for these files. Using this info we can rewrite just the files that got messed up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "out_path = '/home/jsignell/erddapData/Cloud_to_Ground_Lightning/'\n",
    "little = []\n",
    "for fname in os.listdir(out_path):\n",
    "    if os.stat(out_path+fname).st_size <8000:\n",
    "        little.append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "new_path = '/run/media/jsignell/WRF/Data/LIGHT/raw/'\n",
    "fnames = []\n",
    "for fname in os.listdir(new_path):\n",
    "    for l in little:\n",
    "        t = pd.Timestamp(l.partition('.')[0].replace('_','-'))\n",
    "        if '{y}.{doy:03d}'.format(y=t.year, doy=t.dayofyear) in fname:\n",
    "            fnames.append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname = l[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(old_path+fname, delim_whitespace=True, header=None, names=['D', 'T','lat','lon','amplitude','strokes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00:00:00'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['T'][7430175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(7430175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = pd.to_datetime(df['D']+' '+df['T'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[['time', 'lat', 'lon', 'amplitude', 'strokes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>amplitude</th>\n",
       "      <th>strokes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-08-01</td>\n",
       "      <td>39.032</td>\n",
       "      <td>-119.729</td>\n",
       "      <td>-14.7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-08-01</td>\n",
       "      <td>36.763</td>\n",
       "      <td>-102.928</td>\n",
       "      <td>-14.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-08-01</td>\n",
       "      <td>33.577</td>\n",
       "      <td>-84.241</td>\n",
       "      <td>-34.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-08-01</td>\n",
       "      <td>41.409</td>\n",
       "      <td>-99.739</td>\n",
       "      <td>-9.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-08-01</td>\n",
       "      <td>38.925</td>\n",
       "      <td>-102.605</td>\n",
       "      <td>-15.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        time     lat      lon  amplitude  strokes\n",
       "0 2009-08-01  39.032 -119.729      -14.7        6\n",
       "1 2009-08-01  36.763 -102.928      -14.9        1\n",
       "2 2009-08-01  33.577  -84.241      -34.9        2\n",
       "3 2009-08-01  41.409  -99.739       -9.6        1\n",
       "4 2009-08-01  38.925 -102.605      -15.8        2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>amplitude</th>\n",
       "      <th>strokes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [time, lat, lon, amplitude, strokes]\n",
       "Index: []"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.time.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['strokes'] = df['strokes'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "724017    503/12/08\n",
       "Name: strokes, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.strokes[df.strokes == '503/12/08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(724017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-08-01\n",
      "2009-08-02\n",
      "2009-08-03\n",
      "2009-08-04\n",
      "2009-08-05\n",
      "2009-08-06\n",
      "2009-08-07\n",
      "2009-08-08\n",
      "2009-08-09\n",
      "2009-08-10\n",
      "2009-08-11\n",
      "2009-08-12\n",
      "2009-08-13\n",
      "2009-08-14\n",
      "2009-08-15\n",
      "2009-08-16\n",
      "2009-08-17\n",
      "2009-08-18\n",
      "2009-08-19\n",
      "2009-08-20\n",
      "2009-08-21\n",
      "2009-08-22\n",
      "2009-08-23\n",
      "2009-08-24\n",
      "2009-08-25\n",
      "2009-08-26\n",
      "2009-08-27\n",
      "2009-08-28\n",
      "2009-08-29\n",
      "2009-08-30\n",
      "2009-08-31\n"
     ]
    }
   ],
   "source": [
    "days = np.unique(df.time.apply(lambda x: x.date()))\n",
    "for day in days:\n",
    "    df0 = df[(df.time >= day) & (df.time < day+pd.DateOffset(1))]\n",
    "    df0 = df0.reset_index()\n",
    "    df0.index.name = 'record'\n",
    "    write_day(df0, out_path)\n",
    "    print day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nflash2010.336_daily_v3_lit.raw\n",
      "Nflash2010.350_daily_v3_lit.raw\n",
      "Nflash2011.048_daily_v3_lit.raw\n",
      "Nflash2010.304_daily_v3_lit.raw\n",
      "Nflash2010.310_daily_v3_lit.raw\n",
      "Nflash2010.311_daily_v3_lit.raw\n",
      "Nflash2010.312_daily_v3_lit.raw\n",
      "Nflash2010.313_daily_v3_lit.raw\n",
      "Nflash2010.318_daily_v3_lit.raw\n",
      "Nflash2010.323_daily_v3_lit.raw\n",
      "Nflash2010.324_daily_v3_lit.raw\n",
      "Nflash2010.325_daily_v3_lit.raw\n",
      "Nflash2010.330_daily_v3_lit.raw\n",
      "Nflash2010.331_daily_v3_lit.raw\n",
      "Nflash2010.332_daily_v3_lit.raw\n",
      "Nflash2010.335_daily_v3_lit.raw\n",
      "Nflash2010.337_daily_v3_lit.raw\n",
      "Nflash2010.338_daily_v3_lit.raw\n",
      "Nflash2010.343_daily_v3_lit.raw\n",
      "Nflash2010.344_daily_v3_lit.raw\n",
      "Nflash2010.349_daily_v3_lit.raw\n",
      "Nflash2010.351_daily_v3_lit.raw\n",
      "Nflash2010.355_daily_v3_lit.raw\n",
      "Nflash2010.356_daily_v3_lit.raw\n",
      "Nflash2010.357_daily_v3_lit.raw\n",
      "Nflash2010.361_daily_v3_lit.raw\n",
      "Nflash2010.362_daily_v3_lit.raw\n",
      "Nflash2011.003_daily_v3_lit.raw\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "new_path = '/run/media/jsignell/WRF/Data/LIGHT/raw/'\n",
    "out_path = '/home/jsignell/erddapData/Cloud_to_Ground_Lightning/'\n",
    "\n",
    "def new_files(path, fname, out_path):\n",
    "    df = pd.read_csv(path+fname, delim_whitespace=True, header=None, parse_dates={'time':[0,1]})\n",
    "    df = df.drop(5, axis=1)\n",
    "        \n",
    "    df.columns = ['time', 'lat', 'lon', 'amplitude','strokes',\n",
    "                  'semimajor','semiminor','ratio','angle','chi_squared','nsensors','cloud_ground']\n",
    "    df.index.name = 'record'\n",
    "    \n",
    "    attrs = {'semimajor': {'long_name': 'Semimajor Axis of 50% probability ellipse for each flash',\n",
    "                           'units': 'km'},\n",
    "             'semiminor': {'long_name': 'Semiminor Axis of 50% probability ellipse for each flash',\n",
    "                           'units': 'km'},\n",
    "             'ratio': {'long_name': 'Ratio of Semimajor to Semiminor'},\n",
    "             'angle': {'long_name': 'Angle of 50% probability ellipse from North',\n",
    "                       'units': 'Deg'},\n",
    "             'chi_squared': {'long_name': 'Chi-squared value of statistical calculation'},\n",
    "             'nsensors': {'long_name': 'Number of sensors reporting the flash'},\n",
    "             'cloud_ground': {'long_name': 'Cloud_to_Ground or In_Cloud Discriminator'}}\n",
    "\n",
    "\n",
    "    ds = df.to_xarray()\n",
    "    ds.set_coords(['time','lat','lon'], inplace=True)\n",
    "    if df.shape[0] < 5:\n",
    "        chunk=1\n",
    "    else:\n",
    "        chunk = min(df.shape[0]/5, 1000)\n",
    "    for k, v in attrs.items():\n",
    "        ds[k].attrs.update(v)\n",
    "        if k == 'cloud_ground':\n",
    "            ds[k].encoding.update({'dtype': 'S1'})\n",
    "        elif k == 'nsensors':\n",
    "            ds[k].encoding.update({'dtype': np.int32, 'chunksizes':(chunk,),'zlib': True})\n",
    "        else:\n",
    "            ds[k].encoding.update({'dtype': np.double,'chunksizes':(chunk,),'zlib': True})\n",
    "\n",
    "    ds.amplitude.attrs.update({'units': 'kA',\n",
    "                               'long_name': 'Polarity and strength of strike'})\n",
    "    ds.amplitude.encoding.update({'dtype': np.double,'chunksizes':(chunk,),'zlib': True})\n",
    "    ds.strokes.attrs.update({'long_name': 'multiplicity of flash'})\n",
    "    ds.strokes.encoding.update({'dtype': np.int32,'chunksizes':(chunk,),'zlib': True})\n",
    "    ds.lat.attrs.update({'units': 'degrees_north',\n",
    "                         'axis': 'Y',\n",
    "                         'long_name': 'latitude',\n",
    "                         'standard_name': 'latitude'})\n",
    "    ds.lat.encoding.update({'dtype': np.double,'chunksizes':(chunk,),'zlib': True})\n",
    "    ds.lon.attrs.update({'units': 'degrees_east',\n",
    "                         'axis': 'X',\n",
    "                         'long_name': 'longitude',\n",
    "                         'standard_name': 'longitude'})\n",
    "    ds.lon.encoding.update({'dtype': np.double,'chunksizes':(chunk,),'zlib': True})\n",
    "    ds.time.encoding.update({'units':'seconds since 1970-01-01', \n",
    "                             'calendar':'gregorian',\n",
    "                             'dtype': np.double,'chunksizes':(chunk,),'zlib': True})\n",
    "\n",
    "    ds.attrs.update({ 'title': 'Cloud to Ground Lightning',\n",
    "                      'institution': 'Data from NLDN, hosted by Princeton University',\n",
    "                      'references': 'https://ghrc.nsstc.nasa.gov/uso/ds_docs/vaiconus/vaiconus_dataset.html',\n",
    "                      'featureType': 'point',\n",
    "                      'Conventions': 'CF-1.6',\n",
    "                      'history': 'Created by Princeton University Hydrometeorology Group at {now} '.format(now=pd.datetime.now()),\n",
    "                      'author': 'jsignell@princeton.edu',\n",
    "                      'keywords': 'lightning'})\n",
    "\n",
    "    date = df.time[len(df.index)/2]\n",
    "    ds.to_netcdf('{out_path}{y}_{m:02d}_{d:02d}.nc'.format(out_path=out_path, y=date.year, m=date.month, d=date.day), \n",
    "                 format='netCDF4', engine='netcdf4')\n",
    "\n",
    "for fname in fnames:\n",
    "    try:\n",
    "        new_files(new_path, fname, out_path)\n",
    "        print fname\n",
    "    except:\n",
    "        f = open('messed_up_new_files.txt', 'a')\n",
    "        f.write(fname+'\\n')\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor fname in os.listdir(old_path):\\n    try:\\n        old_files(old_path, fname, out_path)\\n    except:\\n        f = open('messed_up_old_files.txt', 'a')\\n        f.write(fname+'\\n')\\n        f.close()\\n\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "old_path = '/run/media/jsignell/WRF/Data/LIGHT/Data_1991-2009/'\n",
    "out_path = '/home/jsignell/erddapData/Cloud_to_Ground_Lightning/'\n",
    "    \n",
    "def write_day(df, out_path):\n",
    "\n",
    "    ds = df.drop('index', axis=1).to_xarray()\n",
    "    ds.set_coords(['time','lat','lon'], inplace=True)\n",
    "    \n",
    "    ds.amplitude.attrs.update({'units': 'kA',\n",
    "                               'long_name': 'Polarity and strength of strike'})\n",
    "    ds.amplitude.encoding.update({'dtype': np.double})\n",
    "    ds.strokes.attrs.update({'long_name': 'multiplicity of flash'})\n",
    "    ds.strokes.encoding.update({'dtype': np.int32})\n",
    "    ds.lat.attrs.update({'units': 'degrees_north',\n",
    "                         'axis': 'Y',\n",
    "                         'long_name': 'latitude',\n",
    "                         'standard_name': 'latitude'})\n",
    "    ds.lat.encoding.update({'dtype': np.double})\n",
    "    ds.lon.attrs.update({'units': 'degrees_east',\n",
    "                         'axis': 'X',\n",
    "                         'long_name': 'longitude',\n",
    "                         'standard_name': 'longitude'})\n",
    "    ds.lon.encoding.update({'dtype': np.double})\n",
    "    ds.time.encoding.update({'units':'seconds since 1970-01-01', \n",
    "                             'calendar':'gregorian',\n",
    "                             'dtype': np.double})\n",
    "\n",
    "    ds.attrs.update({ 'title': 'Cloud to Ground Lightning',\n",
    "                      'institution': 'Data from NLDN, hosted by Princeton University',\n",
    "                      'references': 'https://ghrc.nsstc.nasa.gov/uso/ds_docs/vaiconus/vaiconus_dataset.html',\n",
    "                      'featureType': 'point',\n",
    "                      'Conventions': 'CF-1.6',\n",
    "                      'history': 'Created by Princeton University Hydrometeorology Group at {now} '.format(now=pd.datetime.now()),\n",
    "                      'author': 'jsignell@princeton.edu',\n",
    "                      'keywords': 'lightning'})\n",
    "\n",
    "\n",
    "    date = df.time[len(df.index)/2]\n",
    "    \n",
    "    ds.to_netcdf('{out_path}{y}_{m:02d}_{d:02d}.nc'.format(out_path=out_path, y=date.year, m=date.month, d=date.day), \n",
    "                 format='netCDF4', engine='netcdf4')\n",
    "\n",
    "def old_files(path, fname, out_path):\n",
    "    df = pd.read_csv(path+fname, delim_whitespace=True, header=None, names=['D', 'T','lat','lon','amplitude','strokes'],\n",
    "                     parse_dates={'time':[0,1]})\n",
    "    \n",
    "    days = np.unique(df.time.apply(lambda x: x.date()))\n",
    "    for day in days:\n",
    "        df0 = df[(df.time >= day) & (df.time < day+pd.DateOffset(1))]\n",
    "        df0 = df0.reset_index()\n",
    "        df0.index.name = 'record'\n",
    "        write_day(df0, out_path)\n",
    "        \n",
    "'''\n",
    "for fname in os.listdir(old_path):\n",
    "    try:\n",
    "        old_files(old_path, fname, out_path)\n",
    "    except:\n",
    "        f = open('messed_up_old_files.txt', 'a')\n",
    "        f.write(fname+'\\n')\n",
    "        f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    if df0.shape[0] >1000:\n",
    "        chunks={'chunksizes':(1000,),'zlib': True}\n",
    "    else:\n",
    "        chunks={}\n",
    "    for v in ds.data_vars.keys()+ds.coords.keys():\n",
    "        if v =='strokes':\n",
    "            continue\n",
    "        ds[v].encoding.update(chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
